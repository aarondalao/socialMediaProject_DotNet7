building for production

for more information regarding building your apps before publishing:    
    https://vitejs.dev/guide/
    https://vitejs.dev/guide/build.html

before building:

1) if using create-react-app, MIGRATE TO VITE FIRST! Here's the steps:
    a. Install vitejs

        npm install vite @vitejs/plugin-react-swc eslint eslint-plugin-react-hooks eslint-plugin-react-refresh --save-dev

    b. Uninstall react-scripts

        npm uninstall react-scripts

    c. Replace the existing "scripts" section in package.json with the following:

        "scripts": {
          "start": "vite",
          "dev": "vite",
          "build": "tsc && vite build",
          "lint": "eslint . --ext ts,tsx --report-unused-disable-directives --max-warnings 0",
          "preview": "vite preview"
        },

    d. Create vite.config.ts in client root directory

        import {defineConfig} from 'vite';
        import react from '@vitejs/plugin-react-swc';

        export default defineConfig(() => {
        return {
            build: {
                outDir: '../API/wwwroot'
            },
            server: {
                port: 3000
            },
            plugins: [react()]
            }
        })

    e.  Update the tsconfig.json to what we would get if we created a new vite project

        {
          "compilerOptions": {
            "target": "ES2020",
            "useDefineForClassFields": true,
            "lib": ["ES2020", "DOM", "DOM.Iterable"],
            "module": "ESNext",
            "skipLibCheck": true,

            /* Bundler mode */
            "moduleResolution": "bundler",
            "allowImportingTsExtensions": true,
            "resolveJsonModule": true,
            "isolatedModules": true,
            "noEmit": true,
            "jsx": "react-jsx",

            /* Linting */
            "strict": true,
            "noUnusedLocals": true,
            "noUnusedParameters": true,
            "noFallthroughCasesInSwitch": true
          },
          "include": ["src"],
          "references": [{ "path": "./tsconfig.node.json" }]
        }

    f. Create tsconfig.node.json in the client root folder

        {
            "compilerOptions": {
              "composite": true,
              "skipLibCheck": true,
              "module": "ESNext",
              "moduleResolution": "bundler",
              "allowSyntheticDefaultImports": true
            },
            "include": ["vite.config.ts"]
        }

    g. Create .eslintrc.cjs

        module.exports = {
          root: true,
          env: { browser: true, es2020: true },
          extends: [
            // TODO: uncomment following 2 lines and update code to abide by modern linting rules
            // 'eslint:recommended',
            // 'plugin:@typescript-eslint/recommended',
            'plugin:react-hooks/recommended',
          ],
          ignorePatterns: ['dist', '.eslintrc.cjs'],
          parser: '@typescript-eslint/parser',
          plugins: ['react-refresh'],
          rules: {
            'react-refresh/only-export-components': [
              'off',
              { allowConstantExport: true },
            ],
          },
        }

  
    h.  Move the index.html out of the public folder and into the client root directory.   Remove any PUBLIC_URL references

        <!DOCTYPE html>
        <html lang="en">
          <head>
            <meta charset="utf-8" />
            <meta name="viewport" content="width=device-width, initial-scale=1" />
            <meta name="theme-color" content="#000000" />
            <meta
              name="description"
              content="Web site created using vite"
            />
            <title>React App</title>
          </head>
          <body>
            <noscript>You need to enable JavaScript to run this app.</noscript>
            <div id="root"></div>
            <script type='module' src='/src/main.tsx'></script>
          </body>
        </html>

    i.  If you have .env.development or .env.production update these files so they use the vite environment variables:

        .env.development:

        VITE_API_URL=http://localhost:5000/api
        VITE_CHAT_URL=http://localhost:5000/chat

        .env.production

        VITE_API_URL=/api
        VITE_CHAT_URL=/chat

    j.  Create env.d.ts in the client/src directory

        /// <reference types="vite/client" />

        interface ImportMetaEnv {
            readonly VITE_API_URL: string
            readonly VITE_CHAT_URL: string
            // more env variables...
          }

          interface ImportMeta {
            readonly env: ImportMetaEnv
          }

    k.  Update the agent.ts to use these:

        axios.defaults.baseURL = import.meta.env.VITE_API_URL;

        axios.interceptors.response.use(async response => {
            if (import.meta.env.DEV) await sleep(1000);
            const pagination = response.headers['pagination'];

    l.  Update the commmentStore.ts

        .withUrl(import.meta.env.VITE_CHAT_URL + '?activityId=' + activityId, {

    m. Update index.tsx and remove 'reportWebVitals()'
  
    n. In React 17 you no longer need to import react when writing JSX.  Any imports for 'React' can be removed from all 
    files that have this.

2) update all packages that you're using in the client app to its non-breaking latest versions
    delete the unwanted packages in package.json

    commands that can help:
    npm outdated
    npm install

3) lint your code / do code keeping
    this assumes that you've installed the eslint package and uncommented lines of codes in the .eslintrc.cjs files
    if not, do those first.

    then install these packages:
        npm i @typescript-eslint/parser
        npm i @typescript-eslint/eslint-plugin

    npm commands that can help:

    npm run lint

    then edit out all files that violates the linting rules!
  

CLIENT SIDE BUILDING PROCESS

1) in the client app directory, run this command to run build:

    npm run build

  if there's any errors, fix it.
  else, it will generate a wwwroot folder at the API directory

Run the app in a Kestrel server
  the client app is inside the app now but the API can't serve static contents so this will be configured here.

1) at the Program.cs file, add this middleware 

    app.UseDefaultFiles();
    app.UseStaticFiles();

2) configure the routing of the react application as its own, not the route that is strictly for the API. if the API
does not know the routes other than the configure routes at the MapController and the MapHub, we will need a fallback 
controller for this.

  a) create a FallbackController.cs, delete the other usings and the derive the class to Controller, 
  not the BaseAPIController. then add a route endpoint 

    [AllowAnonymous]
  
    then create an endpoint:

    public IActionResult Index()

  that will return a PhysicalFile and inside the open close parenthesis:

    Path.Combine(Directory.GetCurrentDirectory(), "wwwroot", "index.html"), "text/HTML"

  b) go back to program.cs and add the new middleware 
      
      app.MapFallbackToController("Index", "Fallback");

    where Index is the endpoint that we created in the FallbackController 
    and Fallback is the name of the created controller.
  
3) test the changes either by restarting your API localhost server or starting it
  you should be able to see the production build of the application when using localhost:{YOUR_API_PORT},
  not the localhost:{YOUR_CLIENT_PORT}

MIGRATING TO POSTGRES

1) docker run --name dev -e POSTGRES_USER=admin -e POSTGRES_PASSWORD={MY_POSTGRES_PASSWORD} 
-p 5432:5432 -d postgres:latest

  where:
    -p = Ports used for postgres
    -d => detach mode. docker will run it in the background
    --name = name of the container
    -e  = environment
    {MY_POSTGRES_PASSWORD} => check the value of this password at my key storage

2) run your container

3) in vs code, delete your migrations folder and drop the database by:

  dotnet ef database drop -p Persistence -s API

  where

  -p is shortcut for Project to use. note upon executing a command with this option will be applied to the current 
    working directory
  -s is shortcut for startup project.

  ADDITIONAL! :
  a) change the DateTime.Now into DateTime.UtcNow all throughout the app! key locations:
    * ActivityDto.cs
    * ActivityParams.cs
    * CommentDto.cs
    * TokenService.cs
    * ListActivities.cs
    * UserActivityDto.cs
    * Activity.cs
    * Comment.cs
    * seed.cs

  b) before we commence switching from SQLite to PosgreSQL DB, we need to make sure that our code will run properly,
    particularly about the difference on how these two 2 DB's handle date and time. SQLite does not have a native 
    DateTime.UtcNow method unlike PostgreSQL.
    
    at the commentStore.ts, have a look at the createHubConnection method and change the
    new Date(comment.createdAt + 'Z')
    into
    new Date(comment.createdAt)
    
4) install a new package in Nuget Gallery and look for the package named:
    
    "Npgsql.EntityFrameworkCore.PostgreSQL" by Shay Rojansky, Austin Drenski, et.al

  BUT make sure the version of your RUNTIME (check via dotnet --version or dotnet --info) MATCHES WITH THE VERSION 
  NUMBER OF THIS PACKAGE.

  after a successful installation, go to terminal and execute dotnet restore to restore the new changes to our project.
  
5) at the appsettings.Development.json, change the connectionString to the equivalent connection strings for PostgreSQL

  "Server=localhost; Port=5432; User Id=admin; Password= {MY_POSTGRES_PASSWORD}; Database=how2club;"

  note: check the MY_POSTGRES_PASSWORD at my usual key folder

6) at the applicationServiceExtensions.cs, at the services.AddDbContext<DataContext>(options => {}), 
replace UseSqliteinto UseNpgsql

  options.UseNpgsql(config.GetConnectionString("DefaultConnection"));

7) stop the API server the create a new migrations by

  dotnet ef migrations add <NameOfMigration> InitialPostgresMigration -p Persistence -s API

8) go to API directory and execute at the terminal dotnet run to see if the table creation is successful into the
 database. 

  addtional:
  install the extension PostgreSQL in vs code marketplace



PUBLISHING THE APP + DB PART 1

Heroku does not provide free services anymore (bummer) and im paying one instance there as well, so instead will use 
flyio + docker or AWS ECS + docker to ship my apps to the cloud. first, we'll dockerise the app

1) create a dockerfile at the root folder of the project. on the file write this:

    FROM mcr.microsoft.com/dotnet/sdk:7.0 AS build-env
    WORKDIR /app

    COPY "Reactivities.sln" "Reactivities.sln"
    COPY "API/API.csproj" "API/API.csproj"
    COPY "Application/Application.csproj" "Application/Application.csproj"
    COPY "Domain/Domain.csproj" "Domain/Domain.csproj"
    COPY "Infrastructure/Infrastructure.csproj" "Infrastructure/Infrastructure.csproj"
    COPY "Persistence/Persistence.csproj" "Persistence/Persistence.csproj"

    RUN dotnet restore "Reactivities.sln"

    COPY . .

    RUN dotnet publish -c Release -o out

    #build a runtime image
    FROM mcr.microsoft.com/dotnet/aspnet:7.0
    COPY --from=build-env /app/out .
    ENTRYPOINT [ "dotnet", "API.dll" ]


    where:
    FROM -> this will set the base image that we want to start from
    mcr.microsoft.com/dotnet/sdk:7.0 -> the image that we want. here's the link to the  image: 
      https://hub.docker.com/_/microsoft-dotnet-sdk/
    AS -> a subsequent task to be used for the base image
    build-env -> this represents the base layer of the new image, a starting point
    WORKDIR -> the working path that we're going to use inside the Docker container. take note that THIS IS INSIDE THE 
      DOCKER CONTAINER NOT OUR FILE SYSTEM/ OUR LOCAL MACHINE
    COPY -> copy files or folder from source to destination
    RUN -> execute the commands on top of the current image
    -c -> configuration to publish for
    -o -> the output directory to place the plublished items
    ENTRYPOINT -> this configures the start up executable inside the container 

2) create a new file in the root folder named as .dockerignore and we're going to ignore
  bin and objs folders

    **/bin
    **/obj

  after this, open the appsettings.json and replace the value of the Server in the connection string with this: 

    "Server=host.docker.internal;...omitted for brevity"

  then save the file

3) stop the application if its running then in the terminal, go to root folder and run this command to build our image:

  docker build -t aarondalao/how2club .
  
  where:
  -t -> target name. this includes the username in dockerhub plus / then the name of of the image

 4) run the new image !
    docker run --rm -it -p 8080:80 aarondalao/how2club

    where:
    --rm -> remove the image from the container once we're finished
    --it -> interactive mode. so we can see the dotnet output just as we're running it locally in developer mode
    -p -> ports to use. docker will use port 90 for the webapp and we'll be able to access it via port 8080 
  
    now test the webapp via localhost:8080

  5) push the image to dockerhub!
    but first login to your docker account. you can do that in terminal by:
      docker login
    
    then run this in the terminal:

      docker push {CONTAINER_NAME|IMAGE_NAME}:latest

    lastly, check your repo in dockerhub.com

PUBLISHING THE WEBAPP + DB PART 2
  now lets have a look at the hosting site side of things, for this sample run ill use flyio since heroku is not free anymore
  (or until ive researched enough resources to deploy apps on AWS(either via EC2, Elastic Beanstalk, VMWare on cloud, 
  ECS, EKS, Fargate or Lambda )).

resource: https://fly.io/docs/speedrun/

1) install flyctl, the cli of flyio
  mac: via homebrew package manager
    brew install flyctl
    or 
    run the script
    curl -L https://fly.io/install.sh | sh
  linux: same thing
    curl -L https://fly.io/install.sh | sh
  windows: in powershell
    pwsh -Command "iwr https://fly.io/install.ps1 -useb | iex"

2) setup your login creds, and launch your fly.io instance
  after installation, we can sign up to Fly.io via terminal. sweet mother of potataus

    fly auth signup

    notes:  
    source -> reads and executes command from the file specified as its argument in the current shell environment
          -> useful to load functions variables and configuration files into shell scripts
    nano -> a command line text editor for Unix and Linux distros
    more info on bash shell scripting: https://wiki.archlinux.org/title/Bash#Configuration_files
  
  then sign in

    fly auth sign in

  after that, youll be able to see that youve successfully logged in. execute

    fly launch --image {IMAGE_NAME}
    note:
      {IMAGE_NAME} => MUST BE EXACTLY THE SAME AS THE DOCKER IMAGE UPLOADED IN YOUR DOCKERHUB!

  to generate a fly.toml file. this file will be used to configure our cluster in the flyio cloud. 

3) set up environment variables and secrets to fly

  have a look at the internal_port = 8080. This will be the por that our app is going to use and fly.IO will
  use this internally as well, but the configuration of our Docker image is automatically set to its default for prod app,
  which is port 80. we need to tell our webapp that we want to run the app on Port 80.

  open up the newly generated fly.toml file and then add on these configurations:

  [env]
    ASPNETCORE_URLS="http://+:8080"

  make this as accurate as possible, if not, fly.io will run into some health checks against the port and it will 
  either eventually times out or fails with considerably vague non-specific error stack traces.

  set up our cloudinary api settings:
    Cloudinary__CloudName="{CLOUD_NAME}"
    Cloudinary__ApiKey="{CLOUD_API_KEY}"

  NOTE:
    * if having trouble finding the value for those keys, please have a look at the  keys at appsettings file. 
    * THE NAME ARE DOUBLE UNDERSCORE, NOT SINGLE

  then,we need to store our token key and the cloudinary key into production. in order for us to brows and store our 
  secret files to the flyio, we can use this command:

    fly secrets list
    fly secrets set {TOKEN_NAME_TO_BE_STORED}

    NOTE: 
    * i have changed my token_key password, from a passphrase into a very long password
    * the key that we're storing needs to be EXACTLY THE SAME NAME from the one we just build and deployed to
      docker image BUT THE VALUE SHOULD NOT INCLUDE THE OPEN AND CLOSE COLONS.

  this will stage the secrets into our deployment.
  
  next, we need to edit our dockerfile to expose our port that will be used as an entry point to the webapp for the 
  health checks from Fly.IO : 

    EXPOSE 8080
  
  Connect the webapp instance in the FLY.IO cloud to the DB(postgres) instance 

4) go to applicationServiceExtensions.cs, then change the connection string by using the DATABASE_URL from fly secrets 
  list:

    services.AddDbContext<DataContext>(options =>
    {
      var env = Environment.GetEnvironmentVariable("ASPNETCORE_ENVIRONMENT");
      string connStr;
      // Depending on if in development or production, use either FlyIO
      // connection string, or development connection string from env var.
      if (env == "Development")
      {
          // Use connection string from file.
          connStr = config.GetConnectionString("DefaultConnection");
      }
      else
      {
          // Use connection string provided at runtime by FlyIO.
          var connUrl = Environment.GetEnvironmentVariable("DATABASE_URL");
          // Parse connection URL to connection string for Npgsql
          connUrl = connUrl.Replace("postgres://", string.Empty);
          var pgUserPass = connUrl.Split("@")[0];
          var pgHostPortDb = connUrl.Split("@")[1];
          var pgHostPort = pgHostPortDb.Split("/")[0];
          var pgDb = pgHostPortDb.Split("/")[1];
          var pgUser = pgUserPass.Split(":")[0];
          var pgPass = pgUserPass.Split(":")[1];
          var pgHost = pgHostPort.Split(":")[0];
          var pgPort = pgHostPort.Split(":")[1];
          var updatedHost = pgHost.Replace("flycast", "internal");
          connStr = $"Server={updatedHost};Port={pgPort};User Id={pgUser};Password={pgPass};Database={pgDb};";
      }
      // Whether the connection string came from the local development configuration file
      // or from the environment variable from FlyIO, use it to set up your DbContext.
      options.UseNpgsql(connStr);
    });

5) after the changes made, rebuild the docker image for our webapp, then push it to dockerhub.
6) the final command:
    fly deploy

CI/CD (Continuos Integration / Continuous Delivery) BASICS part 1

so far I've managed to deploy my production webapp successfully, but thats only because im using linux distro of ubuntu 
with an intel cpu (amd64 architecture). if i used arm64 it would be a different situation. what if i am in a 
"hipster-caramel-latte-with-organic-oat-milk-mac-user" situation? this would be my go to approach:

introducing github actions! 

we are particularly looking at this action which was created by docker: https://github.com/docker/build-push-action
essentially this would push a new image to docker whenever we successfully pushed changes to github. now we are NOT
going to push things to dockerhub automatically as we still need to do pull requests before merging to main. Also we are
not going to add the docker/build-push-action action directy to github browser as this can disrupt my branches in github 
(so far i haven't pushed any changes since client-side linting/building, and pre-building the API layer side of 
things.).

so first things first. on the project root folder, create a new folder called .github and on this folder we need to create 
a folder called workflows.on workflows folder, create a file called docker-image.yml. this is the file that we're 
going to use for automating the docker push.

name: ci

on:
  push:
    branches:
      - 'main'

jobs:
  docker:
    runs-on: ubuntu-latest
    steps:
      -
        name: Set up QEMU
        uses: docker/setup-qemu-action@v3
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      -
        name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      -
        name: Build and push
        uses: docker/build-push-action@v5
        with:
          push: true
          tags: user/app:latest

then modify this into:

name: docker-push

on:
  worflow_dispatch:

jobs:
  docker:
    runs-on: ubuntu-latest
    steps:
      -
        name: Setup Docker Buildx
        uses: docker/setup-buildx-action@v3
      -
        name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      -
        name: Build and push
        uses: docker/build-push-action@v3
        with: 
          push: true
          tags: aarondalao/how2club:latest


1) go to github.com, and go to your project settings. from the settings menu, scroll down and find the Secrets/actions feature at the
  security tab. from then on you'll see a green button to the right, click the New Repository Secret to add your dockerhub
  username and token.

2) make changes to your application

3) push your changes to github

ADDITIONAL TASKS:

  REMOVE ALL CONSOLE.LOGS IN THE CLIENT_APP BEFORE PUSHING TO GITHUB!!!!

CI/CD BASICS

at the moment, ive managed to set up a github action to build a docker image and push the docker image to hub.docker repo
via triggering it (somewhat automatic, but need to be aware it still has to be done manually). Would it be better if we
also automate our deployment of our app to fly.io ? Here's how:

1) edit your workflows/{YOUR_GITHUB_ACTION_FILE}.yml with this:

  jobs:
    deploy:
      name: Deploy app
      runs-on: ubuntu-latest
      steps:
        - uses: actions/checkout@v3
        - uses: superfly/flyctl-actions/setup-flyctl@master
        - run: flyctl deploy --remote-only
          env:
            FLY_API_TOKEN: ${{ secrets.FLY_API_TOKEN }}

    note: can add name or on at before declaring a job.

2) on the .yml file, the docker build-push job MUST BE FINISHED first before executing the fly deploy job. to do this,
  add the

      needs: {NAME_OF_THE_FIRST_JOB}

  before the 2nd deploy. Github action tends to run in parallel otherwise stated. and be careful with the indetation of
  yml files or else it will cause errors along the building process.


3) add the fly.io environment api token before THE JOBS LINE in the yml file

    env:
      FLY_API_TOKEN: ${{ secrets.FLY_API_TOKEN }}

  then go to your repo in github and add the new secret api token for fly.io ath the secrets and variables/actions.
  but what is our FLY_API_TOKEN ? you can get that by going to terminal (assuming you are logged in terminal as well,
  otherwise, execute a fly auth login) then type 
  
      flyctl auth token

  paste the token to the value of the new secret in Actions secrets and variables.
      

more resources: https://fly.io/docs/app-guides/continuous-deployment-with-github-actions/

Optional :

TROUBLESHOOTING DEPLOYMENTS IN FLY.IO

As it stands at the moment, im having trouble accessing the app because it's throwing a status code 500 server error but
there's not much detailed information on the error message thrown by the prod server. How will i start fixing this?

these are some of the ways to get infomation:

* note:
  DO NOT DEBUG IN PROD. DEVELOPMENT HAS MUCH MORE TOOLS THAT CAN BE USED FOR DEBUGGING RATHER THAN PROD. MAKE SURE 
  EVERYTHING IN DEV ARE TESTED AND PASSED BEFORE PUSHING TO PROD! 

1) on fly.io dashboard, click your production app, and on the left sidebar, click the monitoring. it should pop the infos
in on the current processes done by the docker image. 

2) almost the same thing but this time you can access the monitoring via the terminal by:
  a) login via terminal
        fly auth login
  b) confirm your email address
  c) type in the terminal:
        fly logs -a {APP_NAME}

        WHERE:
        -a -> app string, the name of the app to be used



 
